{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# sd+ipadapterPromptsuz"
      ],
      "metadata": {
        "id": "40OaBQm166yP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# SD3.5-LARGE + IP-Adapter (Img2Img) — PROMPTSUZ, UPSCALE/RESIZE YOK\n",
        "# =========================\n",
        "\n",
        "# --- Kurulum (Colab ise aç) ---\n",
        "# !pip -q install -U \"diffusers>=0.33.0\" \"transformers>=4.43.0\" accelerate safetensors pillow opencv-python bitsandbytes\n",
        "\n",
        "# from huggingface_hub import login\n",
        "# login()  # HF token gir\n",
        "\n",
        "import os, glob, gc, contextlib, torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from itertools import islice\n",
        "\n",
        "from diffusers import StableDiffusion3Img2ImgPipeline\n",
        "from transformers import SiglipVisionModel, SiglipImageProcessor\n",
        "\n",
        "# -------- Bellek/performans --------\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64,expandable_segments:True\"\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# -------- Model/Adapter ID'leri --------\n",
        "MODEL_ID         = \"stabilityai/stable-diffusion-3.5-large\"\n",
        "IPADAPTER_ID     = \"InstantX/SD3.5-Large-IP-Adapter\"\n",
        "IPADAPTER_REV    = \"f1f54ca369ae759f9278ae9c87d46def9f133c78\"\n",
        "IMAGE_ENCODER_ID = \"google/siglip-so400m-patch14-384\"\n",
        "\n",
        "# -------- Promptsuz --------\n",
        "BASE_PROMPT = \"\"   # boş -> metin etkisi yok\n",
        "NEG_PROMPT  = \"\"   # boş -> negatif istem yok\n",
        "\n",
        "# -------- Parametreler --------\n",
        "GUIDANCE           = 1.0      # CFG kapalı gibi davranır\n",
        "STEPS              = 24\n",
        "STRENGTH           = 0.24     # 0.18–0.30 arası, daha düşük -> giriş yapısı daha çok korunur\n",
        "IP_SCALE           = 0.90     # 0.8–1.0 kompozisyonu görselden güçlü alır\n",
        "USE_EMBEDS         = True\n",
        "VARIANTS_PER_IMAGE = 1\n",
        "BASE_SEED          = 2025\n",
        "\n",
        "# -------- Boyut işlemleri --------\n",
        "\n",
        "PAD_TO_MULTIPLE_OF_16 = True\n",
        "\n",
        "def list_images(folder: str):\n",
        "    exts = (\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\", \".tif\", \".tiff\")\n",
        "    files = []\n",
        "    for e in exts:\n",
        "        files += glob.glob(os.path.join(folder, f\"*{e}\"))\n",
        "    return sorted(files)\n",
        "\n",
        "def pad_to_mult16(img: Image.Image) -> Image.Image:\n",
        "    if not PAD_TO_MULTIPLE_OF_16:\n",
        "        return img\n",
        "    w, h = img.size\n",
        "    pad_w = (16 - (w % 16)) % 16\n",
        "    pad_h = (16 - (h % 16)) % 16\n",
        "    if pad_w == 0 and pad_h == 0:\n",
        "        return img\n",
        "    # Kenarlara minimal padding (refleksiyon doğal durur)\n",
        "    im = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
        "    top, bottom = 0, pad_h\n",
        "    left, right = 0, pad_w\n",
        "    im_pad = cv2.copyMakeBorder(im, top, bottom, left, right, borderType=cv2.BORDER_REFLECT_101)\n",
        "    im_pad = cv2.cvtColor(im_pad, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(im_pad)\n",
        "\n",
        "# -------- IO --------\n",
        "CANDIDATE_DIRS = [\"/content/drive/MyDrive/boat_dataset/fullphoto\"]\n",
        "INPUT_DIR = next((d for d in CANDIDATE_DIRS if os.path.isdir(d)), None)\n",
        "assert INPUT_DIR, f\"Girdi klasörü bulunamadı. Şunlardan biri olmalı: {CANDIDATE_DIRS}\"\n",
        "\n",
        "OUT_DIR = \"/content/boat_out_sd35_i2i_ip4\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Kaç görsel işlenecek (None: hepsi)\n",
        "max_images = None\n",
        "\n",
        "# -------- SigLIP --------\n",
        "feature_extractor = SiglipImageProcessor.from_pretrained(IMAGE_ENCODER_ID)\n",
        "image_encoder = SiglipVisionModel.from_pretrained(IMAGE_ENCODER_ID, torch_dtype=torch.float16)\n",
        "\n",
        "# -------- Pipeline --------\n",
        "pipe_kwargs = dict(\n",
        "    torch_dtype=torch.float16,\n",
        "    variant=\"fp16\",\n",
        "    use_safetensors=True,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "# Text encoder 3'ü düşür (VRAM kazanımı)\n",
        "DROP_T5 = True\n",
        "if DROP_T5:\n",
        "    pipe_kwargs.update(text_encoder_3=None, tokenizer_3=None)\n",
        "\n",
        "print(\"Model yükleniyor:\", MODEL_ID)\n",
        "pipe = StableDiffusion3Img2ImgPipeline.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    feature_extractor=feature_extractor,  # IP-Adapter için processor\n",
        "    image_encoder=image_encoder,          # SigLIP encoder\n",
        "    **pipe_kwargs,\n",
        ")\n",
        "\n",
        "# Varsa yine de T5'i at\n",
        "if DROP_T5:\n",
        "    if hasattr(pipe, \"text_encoder_3\"): pipe.text_encoder_3 = None\n",
        "    if hasattr(pipe, \"tokenizer_3\"):    pipe.tokenizer_3 = None\n",
        "\n",
        "# Güvenlik denetçisini kapat (opsiyonel)\n",
        "if hasattr(pipe, \"safety_checker\"): pipe.safety_checker = None\n",
        "if hasattr(pipe, \"requires_safety_checker\"): pipe.requires_safety_checker = False\n",
        "\n",
        "# IP-Adapter yükle\n",
        "pipe.load_ip_adapter(\n",
        "    IPADAPTER_ID,\n",
        "    weight_name=\"ip-adapter.bin\",\n",
        "    revision=IPADAPTER_REV,\n",
        ")\n",
        "pipe.set_ip_adapter_scale(IP_SCALE)\n",
        "\n",
        "# VAE/attention optimizasyonları\n",
        "pipe.vae.enable_slicing()\n",
        "pipe.vae.enable_tiling()\n",
        "try:\n",
        "    pipe.enable_sdpa()\n",
        "except Exception:\n",
        "    pipe.enable_attention_slicing()\n",
        "\n",
        "# CPU offload\n",
        "if device.type == \"cuda\":\n",
        "    pipe.enable_model_cpu_offload()\n",
        "else:\n",
        "    pipe.to(\"cpu\")\n",
        "\n",
        "try:\n",
        "    pipe.set_progress_bar_config(disable=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def print_devices(pipe):\n",
        "    try: print(\"unet     :\", next(pipe.unet.parameters()).device)\n",
        "    except: pass\n",
        "    try: print(\"vae      :\", next(pipe.vae.parameters()).device)\n",
        "    except: pass\n",
        "    try: print(\"te1      :\", next(pipe.text_encoder.parameters()).device)\n",
        "    except: pass\n",
        "    try: print(\"te2      :\", next(pipe.text_encoder_2.parameters()).device)\n",
        "    except: pass\n",
        "    try: print(\"img_enc  :\", next(pipe.image_encoder.parameters()).device)\n",
        "    except: pass\n",
        "    try:\n",
        "        mod = getattr(pipe, \"ip_adapter\", None) or getattr(pipe, \"image_proj_model\", None)\n",
        "        if mod is not None:\n",
        "            print(\"ip_adapter:\", next(mod.parameters()).device)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print_devices(pipe)\n",
        "\n",
        "# -------- Görselleri topla --------\n",
        "all_imgs = list_images(INPUT_DIR)\n",
        "assert all_imgs, f\"Girdi klasöründe görsel yok: {INPUT_DIR}\"\n",
        "\n",
        "iterable = all_imgs if max_images is None else islice(all_imgs, max_images)\n",
        "total_to_process = len(all_imgs) if max_images is None else min(len(all_imgs), int(max_images))\n",
        "\n",
        "print(f\"Bulunan görsel: {len(all_imgs)}; İşlenecek: {total_to_process}\")\n",
        "print(f\"Çıkış klasörü: {OUT_DIR}\")\n",
        "\n",
        "processed = 0\n",
        "skipped = 0\n",
        "\n",
        "# Autocast bağlamı\n",
        "autocast_ctx = (\n",
        "    torch.autocast(device_type=\"cuda\", dtype=torch.float16)\n",
        "    if device.type == \"cuda\" else contextlib.nullcontext()\n",
        ")\n",
        "\n",
        "for idx, path in enumerate(tqdm(iterable, total=total_to_process, desc=\"Processing\")):\n",
        "    try:\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        print(\"Skip (okuma hatası):\", path, e)\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    # Ölçekleme yok -> sadece 16'ya pad (opsiyonel)\n",
        "    base_img = pad_to_mult16(img)\n",
        "\n",
        "    # Deterministik üretim için seed\n",
        "    g = torch.Generator(device=device).manual_seed(BASE_SEED + idx)\n",
        "    do_cfg = (GUIDANCE > 1.0)  # 1.0'da False\n",
        "\n",
        "    if USE_EMBEDS:\n",
        "        ip_embeds = pipe.prepare_ip_adapter_image_embeds(\n",
        "            ip_adapter_image=base_img,\n",
        "            device=device,\n",
        "            num_images_per_prompt=1,\n",
        "            do_classifier_free_guidance=do_cfg,\n",
        "        )\n",
        "\n",
        "    for k in range(VARIANTS_PER_IMAGE):\n",
        "        with autocast_ctx, torch.inference_mode():\n",
        "            kwargs = dict(\n",
        "                prompt=BASE_PROMPT,          # \"\"\n",
        "                negative_prompt=NEG_PROMPT,  # \"\"\n",
        "                image=base_img,              # ORİJİNAL BOYUT\n",
        "                strength=STRENGTH,\n",
        "                guidance_scale=GUIDANCE,\n",
        "                num_inference_steps=STEPS,\n",
        "                generator=g,\n",
        "            )\n",
        "            if USE_EMBEDS:\n",
        "                kwargs[\"ip_adapter_image_embeds\"] = ip_embeds\n",
        "            else:\n",
        "                kwargs[\"ip_adapter_image\"] = base_img\n",
        "\n",
        "            result = pipe(**kwargs)\n",
        "\n",
        "        out_img = result.images[0]\n",
        "        name = os.path.splitext(os.path.basename(path))[0]\n",
        "        suffix = f\"_PRMPLESS_ORIGSIZE_st{STRENGTH}_ip{IP_SCALE}_gs{GUIDANCE}_s{STEPS}\"\n",
        "        if PAD_TO_MULTIPLE_OF_16:\n",
        "            suffix += \"_PAD16\"\n",
        "        if VARIANTS_PER_IMAGE > 1:\n",
        "            suffix += f\"_v{k+1}\"\n",
        "        out_path = os.path.join(OUT_DIR, f\"{name}{suffix}.png\")\n",
        "        out_img.save(out_path)\n",
        "\n",
        "        del out_img, result\n",
        "\n",
        "    processed += 1\n",
        "    del base_img, img\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "print(f\"Bitti. İşlenen: {processed}, Atlanan: {skipped}, Çıktı klasörü: {OUT_DIR}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396,
          "referenced_widgets": [
            "85aef80d3127496eb893490064c24194",
            "ad6b0bd596c64551aebd678e1c91e288",
            "824dd2e883ae4474b6be1a7e27923d5f",
            "18c94c202ea8406d9178ca9b6d63839f",
            "19cab8caa5f1441386f2c8d3e585334f",
            "9e0e466d7b004ce9ac3032061044947a",
            "9b8740015f984bb9a2ef4482f9cdc457",
            "ca77cf81292d4bf186670dc4bcd2001b",
            "451e790a0f794f3c9a4a1488595c982a",
            "244ad158d89d425db6d28560d3102735",
            "03893609c16a440581832d5c29c54ef9",
            "081883bb9a494038b0c8240541fcf84b",
            "49b183ece2b041b79a9232a5780fd821",
            "5b9d41020f5542bca030d5003a895fc5",
            "a9e224cce1544027bf627edc174919d5",
            "6b0aba60702543798bcd1a987810877f",
            "c60bef28b4de423b9c56050a5f68f669",
            "40770d13b46249d48e7a8d4a3ee52a5a",
            "1258676c27dc42adb3381a3fcfeb2245",
            "2396e3cd2e374e31b130c1b9e3ad8eed",
            "4e66a8cb66d3469686159de68f3e07a7",
            "2ca962e21367417b9c24c619592b1c2e"
          ]
        },
        "id": "FU7eKYHC694f",
        "outputId": "a361780a-861e-4afb-d5dc-566e9993384b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Model yükleniyor: stabilityai/stable-diffusion-3.5-large\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A mixture of fp16 and non-fp16 filenames will be loaded.\n",
            "Loaded fp16 filenames:\n",
            "[text_encoder_3/model.fp16-00001-of-00002.safetensors, text_encoder_3/model.fp16-00002-of-00002.safetensors, text_encoder/model.fp16.safetensors, text_encoder_2/model.fp16.safetensors, text_encoder_3/model.safetensors.index.fp16.json]\n",
            "Loaded non-fp16 filenames:\n",
            "[vae/diffusion_pytorch_model.safetensors, transformer/diffusion_pytorch_model-00001-of-00002.safetensors, transformer/diffusion_pytorch_model.safetensors.index.json, transformer/diffusion_pytorch_model-00002-of-00002.safetensors\n",
            "If this behavior is not expected, please check your folder structure.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85aef80d3127496eb893490064c24194"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "081883bb9a494038b0c8240541fcf84b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vae      : cpu\n",
            "te1      : cpu\n",
            "te2      : cpu\n",
            "img_enc  : cpu\n",
            "Bulunan görsel: 52; İşlenecek: 52\n",
            "Çıkış klasörü: /content/boat_out_sd35_i2i_ip4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 52/52 [22:00<00:00, 25.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bitti. İşlenen: 52, Atlanan: 0, Çıktı klasörü: /content/boat_out_sd35_i2i_ip4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/file6.zip /content/boat_out_sd35_i2i_ip4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eef6qrDM_Qsq",
        "outputId": "e5665fa5-800a-450c-c45c-01c94fc884ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/boat_out_sd35_i2i_ip4/ (stored 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_0895_NIR_Haze_frame410_jpg.rf.e7caf4f8b28f15b1f9983e152b14e543_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1587_VIS_frame585_jpg.rf.c7440040e62549436cd4bf3bea6c1282_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 1%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1587_VIS_frame150_jpg.rf.5f70fb969b4d20b195149ebb4e101139_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_0790_VIS_OB_frame205_jpg.rf.ffb3b1e06b4bbc3c6218575f4de1c5a5_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1584_VIS_frame455_jpg.rf.be55fcaa2115ae121576be9c8d6c6ab3_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1523_NIR_frame525_jpg.rf.e0249e1bee87a2a2136fd6cc7a7957d1_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1587_VIS_frame590_jpg.rf.6fa20ff5643358841031096a022e5715_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1523_NIR_frame340_jpg.rf.a3534fb64aaa433cdb79b427ca9d0930_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_0801_VIS_OB_frame360_jpg.rf.51eda5f348ef87ec50d96999a43fcc90_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_0790_VIS_OB_frame210_jpg.rf.57a67503f2e0c4deace7a12b44bf4bba_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1523_NIR_frame345_jpg.rf.9877017bceea86b3c010b0a5c5c7aa53_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_0790_VIS_OB_frame245_jpg.rf.aaf294a792130a2ae5b277ebf5e3e8df_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_0790_VIS_OB_frame165_jpg.rf.5fa61cf962a8bbf6e03718a3637519ed_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 1%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1523_NIR_frame565_jpg.rf.09132b1f7f03af114e4d3f80e75110d8_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1523_NIR_frame595_jpg.rf.32b6f045056049734170a48609bfa85c_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_0790_VIS_OB_frame250_jpg.rf.cd382286fafded9b6d0e0fbc51a754bb_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_0790_VIS_OB_frame170_jpg.rf.316e3a484e5d1c951c532f840a58bb6e_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 1%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1523_NIR_frame590_jpg.rf.7cebc11468559c9b26385dd41bdd8477_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1523_NIR_frame480_jpg.rf.c4799ba8b787bd08b1899d89191192e6_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1587_VIS_frame145_jpg.rf.8a859bdd90594737afb6531244a36486_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_0895_NIR_Haze_frame355_jpg.rf.a80e30de66e02349f271f63a41a6cdd6_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1587_VIS_frame140_jpg.rf.ee584b8347dcf5e2cbd3db6058aeb4a0_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 1%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1523_NIR_frame300_jpg.rf.9037f4822e9094520af0d6049ec0b52c_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1587_VIS_frame575_jpg.rf.a7edd21202a9db9e16d81960932b164e_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_0895_NIR_Haze_frame365_jpg.rf.aafd4852dc6f7070864ab36da893cfc0_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1587_VIS_frame570_jpg.rf.2040652abec4063572ec5e05471bd281_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 1%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1584_VIS_frame415_jpg.rf.3679db6e90ec2b93783280426236f484_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1523_NIR_frame330_jpg.rf.89b326805e3b6bc6cc4531a435f25056_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1587_VIS_frame560_jpg.rf.ac6f4a34fe990777fb5508b1f9fcd93b_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 1%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1523_NIR_frame530_jpg.rf.67775de1f35a6cceac8a53dafc10a1a1_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1523_NIR_frame570_jpg.rf.997ff58df0f40e8187dc73c9414b2553_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1584_VIS_frame445_jpg.rf.e5d2b8c8cc43c9972bbc8d3335dae1df_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1584_VIS_frame430_jpg.rf.80f7087c49ab7c05bc4f0b709acc144d_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1523_NIR_frame335_jpg.rf.a8a894de3f5c3379f34c972934370b76_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1523_NIR_frame520_jpg.rf.59b899843843e97704fdc0a2ab2f76e1_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1523_NIR_frame305_jpg.rf.b1dfa73a1b3f1bebc36b9fb093634482_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1587_VIS_frame155_jpg.rf.077a599248d7685726143e3a88ff802e_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_0895_NIR_Haze_frame350_jpg.rf.5e28f7ea6fee895584f4e09de7ea7d88_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/background_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1587_VIS_frame130_jpg.rf.7b8559cf3d333245084d1cfde0b628a4_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1587_VIS_frame595_jpg.rf.3c2f2333ffa0e0687e38d0430041325b_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 1%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1587_VIS_frame135_jpg.rf.fc132add17a1294bb2a3e0802ead4825_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1584_VIS_frame460_jpg.rf.055778b76fbf437c620a5e3ecf1fefdc_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1587_VIS_frame580_jpg.rf.95a1ca9861ac22f012eee56ddbfef2ea_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_0895_NIR_Haze_frame405_jpg.rf.a0852873d234c3e83f2b6b56bfacd27a_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1587_VIS_frame565_jpg.rf.7ee4aece3bb874617d8cc411f1351330_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1523_NIR_frame290_jpg.rf.62d2c9928d3c43056f42f0617e02e1a1_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_0801_VIS_OB_frame350_jpg.rf.9bf14242bb4e88e0e62b56443e775b47_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_0895_NIR_Haze_frame360_jpg.rf.767cd559b8c9d2d541669a42416007d8_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1523_NIR_frame295_jpg.rf.8810751381adc3cfba9b1b112e0a38f3_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_1584_VIS_frame410_jpg.rf.81c3b286e0f76bee9f4a5362e3015b20_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n",
            "  adding: content/boat_out_sd35_i2i_ip4/MVI_0801_VIS_OB_frame355_jpg.rf.525a71d1b00e23d46f252c9c8e40b10b_PRMPLESS_ORIGSIZE_st0.24_ip0.9_gs1.0_s24_PAD16.png (deflated 0%)\n"
          ]
        }
      ]
    }
  ]
}